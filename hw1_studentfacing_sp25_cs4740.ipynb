{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmFqIEb7CRO"
      },
      "source": [
        "# Homework 1 Section B: Named Entity Recognition (NER) with HMMs\n",
        "\n",
        "### Milestone Submission Due: February 12, 2025 (11:59PM)\n",
        "### Project Submission Due: February 21, 2025 (11:59PM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7n_cjoN7CRS"
      },
      "source": [
        "## **Logistics**\n",
        "\n",
        "### Notes:\n",
        "  \n",
        "- You will **NOT** be submitting this .ipynb file. Please refer to the submission instructions in both the hw1 pdf shared with you and at the end of this notebook.\n",
        "- Please complete the written questions in the same pdf document where you attempt Section A of the homework.\n",
        "- Do **NOT** add, remove, or modify any imports across python source files. If you have any concerns regarding missing imports, please let course staff know through EdStem before attempting to change anything.\n",
        "- Do **NOT** change any of the function headers and/or specs! The input(s) and output must perfectly match the specs, or else your implementation for any function with changed specs will most likely fail! (for e.g. do not shuffle your data when generating the output.txt file! )\n",
        "- If you decide to create local helper functions, your code must have docstrings/comments documenting the meaning of parameters and important parameter-like variables.\n",
        "- We are recommending python version 3.9+. This is due to compatibility issues with the external dependencies.\n",
        "\n",
        "\n",
        "### Tips:\n",
        "- Pair program the more intensive parts of this assignment! You'll thank yourselves later for the amount of trouble this helps you avoid.\n",
        "- We recommend you start this assignment early and continue incrementally adding onto it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvrfpo597CRU"
      },
      "source": [
        "## Part 0: Environment setup\n",
        "\n",
        "**IMPORTANT:** Read the following file: `vscode-setup.md` to setup your environment for development including setting up VSCode, adding Python extensions, creating virtual environments, and installing dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JelKDZQH7CRU"
      },
      "outputs": [],
      "source": [
        "### AUTORELOAD EXTENSION -- DO NOT MODIFY ###\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7MTlknh7CRV"
      },
      "outputs": [],
      "source": [
        "### IMPORTS -- DO NOT MODIFY ###\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from helpers import apply_smoothing, handle_unknown_words\n",
        "from models import HMM\n",
        "from viterbi import viterbi\n",
        "from validation import evaluate_model, mean_f1, format_output_labels\n",
        "from data_exploration import unzip_data, read_json, stringify_labeled_doc, validate_ner_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIau-lWb7CRV"
      },
      "source": [
        "## Part 1: Data Exploration\n",
        "\n",
        "### Loading the Data\n",
        "\n",
        "The data is stored in a zip file. You can use the following provided function to\n",
        "load the data and preprocess it. Under the hood, this is unzipping the data and reading each of\n",
        "the provided json data files into Python dictionaries. It then further formats the data such that\n",
        "we can accurately train our model from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AG9AZj7CRW"
      },
      "outputs": [],
      "source": [
        "data_zip_path = \"./dataset.zip\"\n",
        "dest_path = \"dataset\"\n",
        "\n",
        "unzip_data(data_zip_path, dest_path) # unzips the data into current directory\n",
        "\n",
        "training_data = read_json(os.path.join(dest_path, \"train.json\"))\n",
        "validation_data = read_json(os.path.join(dest_path, \"val.json\"))\n",
        "test_data = read_json(os.path.join(dest_path, \"test.json\"))\n",
        "\n",
        "training_data['text'] = [sen[:-1] for sen in training_data['text']]\n",
        "validation_data['text'] = [sen[:-1] for sen in validation_data['text']]\n",
        "test_data['text'] = [sen[:-1] for sen in test_data['text']]\n",
        "\n",
        "training_data['NER'] = [sen[:-1] for sen in training_data['NER']]\n",
        "validation_data['NER'] = [sen[:-1] for sen in validation_data['NER']]\n",
        "\n",
        "training_data['index'] = [sen[:-1] for sen in training_data['index']]\n",
        "validation_data['index'] = [sen[:-1] for sen in validation_data['index']]\n",
        "test_data['index'] = [sen[:-1] for sen in test_data['index']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcFDvWtI7CRW"
      },
      "source": [
        "### Looking at the data\n",
        "\n",
        "Since your data files can be large and unwieldy, you can explore the data by\n",
        "writing code. Check out the data format by looking at at keys, and some of the\n",
        "values in the data. You can use the following code to get started:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwCB5gdw7CRW"
      },
      "outputs": [],
      "source": [
        "print(training_data.keys())\n",
        "print(validation_data.keys())\n",
        "print(test_data.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6akC7Fv_7CRW"
      },
      "source": [
        "Notice the test data does not have any labels. This is because you will need to\n",
        "predict them using your models!\n",
        "\n",
        "To get a sense of what your data looks like, check out some samples. Implement\n",
        "the `stringify_labeled_doc` function in `data_exploration.py`, and use it to print\n",
        "out some samples of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEaFvG097CRW"
      },
      "outputs": [],
      "source": [
        "random_index = random.randint(0, len(training_data['text']))\n",
        "text = training_data['text'][random_index]\n",
        "ner = training_data['NER'][random_index]\n",
        "s = stringify_labeled_doc(text, ner)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YMAffHm7CRW"
      },
      "source": [
        "#### Q1.1: Using the `stringify_labeled_doc` function you implemented, print 5 documents (sentences) from the training data which have at least 4 distinct tags (including 'O'). What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPHzp5l07CRX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45c58Vr7CRX"
      },
      "source": [
        "### Validating the data\n",
        "\n",
        "This data looks okay, _right?_ ... _right?_ The reality is that language data is\n",
        "super messy. One option would be to look through every example by hand, but this\n",
        "is impractical. Another option would be to write a program that could check if\n",
        "each example is correct, but if you could do that, you wouldn't need to write\n",
        "the program to do NER tagging in the first place! However, there is a middle\n",
        "ground. If you had a programatic way to check the validity of the data, you\n",
        "could catch some of these issues. Let's try to do that.\n",
        "\n",
        "#### Q1.2: Implement the `validate_ner_sequence` function in `data_exploration.py` Are there any documents in the training data which have invalid labelings? If so, how many are there?\n",
        "\n",
        "Hint: Think about what makes a valid sequence of labels under the BIO tagging scheme.\n",
        "\n",
        "### Data Statistics\n",
        "\n",
        "In order to look at the data in a different way, you can visualize some of its characteristics. Take the example\n",
        "below, which shows the distribution of the number of tokens per document in the training data. You can use the following code to get started:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6VQ_7gK7CRX"
      },
      "outputs": [],
      "source": [
        "doc_lengths = [len(doc) for doc in training_data[\"text\"]]\n",
        "\n",
        "def plot_histogram(data, title, xlabel, ylabel, bins=50):\n",
        "  \"\"\"\n",
        "  Plots a histogram of the data.\n",
        "\n",
        "  Input:\n",
        "    data: List[Int], representing the data to be plotted\n",
        "    title: String, representing the title of the plot\n",
        "    x_label: String, representing the x-axis label\n",
        "    y_label: String, representing the y-axis label\n",
        "    i: Int, representing the figure number\n",
        "  Output:\n",
        "    None\n",
        "  \"\"\"\n",
        "  plt.hist(data, bins=bins, color=\"maroon\")\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "plot_histogram(\n",
        "\t\tdoc_lengths,\n",
        "\t\t\"Distribution of document length\",\n",
        "\t\t\"Document Length\",\n",
        "\t\t\"Count\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvM9s7J97CRX"
      },
      "source": [
        "#### Q1.3: Provide a bar graph giving the token level distribution of NER tags, (O included): e.g. 10% of tokens are B-ORG, 20% of tokens are I-ORG, etc. What do you notice about this distribution? Is this what you might expect? What difficulties might this cause for your models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPhzGY0H7CRX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gitXbGWG7CRX"
      },
      "source": [
        "#### Q1.4: It seems to be that entity tokens are uppercase. Is this correct? Plot a 2x2 matrix displaying the counts, where one dimension represents whether a token is uppercase and the other dimension indicates whether the token is part of an entity. What do you notice about this distribution? Is this what you might expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg1x0IHF7CRX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPlG0lZx7CRX"
      },
      "source": [
        "#### Q1.5: Provide a list of the 10 tokens which are most frequently tagged as part of a named entity. What do you notice about this distribution? What difficulties might this cause for your models?\n",
        "\n",
        "For examples if\n",
        "- tokens = [\"Cornell\", \"University\", \"is\", \"a\", \"university\", \"in\", \"Ithaca\", \",\", \"New\", \"York\"]\n",
        "- tags   = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\", \"I-LOC\"]\n",
        "\n",
        "In this example, there are 3 named entities:\n",
        "- [\"Cornell University\", \"Ithaca\", \"New York\"].\n",
        "\n",
        "However, you are only interested in tokens which are tagged as part of named entities, which would be [\"Cornell\", \"University, \"Ithaca\", \"New\", \"York\"]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVqeORv_7CRX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EuI2qdvtgTJ"
      },
      "source": [
        "#### Q1.6: Provide a scatter plot mapping the length of a document to the number of named entities in that document. Describe what the plot looks like, and what it might mean about the relationship between named entities and document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BylcxFsXtgTJ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_gTE2BI7CRX"
      },
      "source": [
        "#### Q1.7: Make a convincing arugment around a novel insight about the data. In only a few sentences, argue why this insight is important in understanding the data for this task, and support your answer with relevant graphs or statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp8QeBcG7CRX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGonsYOV7CRX"
      },
      "source": [
        "## Part 2: Hidden Markov Model\n",
        "In this part of the assignment, you will:\n",
        "1. Implement code for counting and smoothing of labels and words, as well as unknown word handing, as necessary to support the Viterbi algorithm.\n",
        "2. Build a Hidden Markov Model in accordance with the starter code that has been provided in `models.py`.\n",
        "3. Implement the **Viterbi algorithm**, that can be used to infer token-level labels (identifying the appropriate named entity) for an input document. This process is commonly referred to as **decoding**. Bigram-based Viterbi is $ \\mathcal{O}(sm^2)$ where *s* is the length of the sentence and *m* is the number of tags. Your implementation should have similar efficiency.\n",
        "\n",
        "### Unknown Word Handling\n",
        "---\n",
        "Handling unknown words is essential for improving the robustness of your sequence tagging model. When your model encounters words that were not present in the training data, it might struggle to make accurate predictions.\n",
        "\n",
        "In this section, you will complete `handle_unknown_words(t, documents)` function in `helpers.py` The unknown word handling function replaces infrequently occurring words in the text data with a special \"\\<unk\\>\" token. This ensures that words with low occurrence are not treated as unique and distinct, allowing the model to recognize them as a common unknown category. By doing so, the method helps the model maintain consistent performance when dealing with unfamiliar or rare words, enhancing its ability to handle unseen data effectively.\n",
        "\n",
        "Detailed specs about expected inputs and outputs are outlined in `helpers.py`. It injests the tokenized documents of a corpus (e.g. the training_data[\"text\"] from above) and returns a tokenized document corpus with infrequent words replaced by \"\\<unk\\>\", as well as the resulting vocab.\n",
        "\n",
        "After you complete the function, you may run the following basic test case. Passing this test does NOT guarantee correctness and it is a good idea to write some tests of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "one7Ub_17CRX"
      },
      "outputs": [],
      "source": [
        "t = 0.3  #see helpers.py for information about this parameter means\n",
        "documents = [[\"good\", \"morning\"], [\"is\", \"good\", \"morning\"], [\"hello\"]]\n",
        "expected_new_documents = [[\"good\", \"morning\"], [\"is\", \"good\", \"morning\"], [\"<unk>\"]]\n",
        "expected_vocab = [\"good\", \"morning\", \"is\", \"<unk>\"]\n",
        "new_documents, vocab = handle_unknown_words(t, documents)\n",
        "\n",
        "print(expected_new_documents)\n",
        "print(new_documents)\n",
        "\n",
        "assert expected_new_documents == new_documents\n",
        "assert sorted(expected_vocab) == sorted(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvD6P8Qp7CRY"
      },
      "source": [
        "### Smoothing\n",
        "---\n",
        "Smoothing makes our sequence tagging model more resilient to unexpected words and rare patterns, improving its ability to predict accurately on unseen data.\n",
        "\n",
        "In this section, you will complete `apply_smoothing(k, observation_counts, unique_obs)` function in `helpers.py` file. The function implements an add-k smoothing. It adds a constant value (k) to each count before calculating probabilities, ensuring that no count is zero and that probabilities are never zero. While there are more advanced smoothing techniques, add-k smoothing provides a good starting point for handling the challenges of sparse data in probabilistic modeling.\n",
        "\n",
        "For example, given an unsmoothed frequency count\n",
        "```\n",
        "unsmoothed_counts = {\n",
        "    ('PER', 'John'): 3,\n",
        "    ('PER', 'Google'): 0,\n",
        "    ('PER', 'California'): 0,\n",
        "    ('PER', 'works'): 0,\n",
        "    ('PER', 'at'): 0,\n",
        "    ('ORG', 'Google'): 2,\n",
        "    ('ORG', 'John'): 0,\n",
        "    ('ORG', 'California'): 0,\n",
        "    ('ORG', 'works'): 0,\n",
        "    ('ORG', 'at'): 0,\n",
        "    ('LOC', 'Google'): 0,\n",
        "    ('LOC', 'John'): 0,\n",
        "    ('LOC', 'California'): 1,\n",
        "    ('LOC', 'works'): 0,\n",
        "    ('LOC', 'at'): 0,\n",
        "    ('O', 'Google'): 0,\n",
        "    ('O', 'John'): 0,\n",
        "    ('O', 'California'): 0,\n",
        "    ('O', 'works'): 4,\n",
        "    ('O', 'at'): 5,\n",
        "}\n",
        "```\n",
        "the smoothed add-1 frequency count should be\n",
        "```\n",
        "smoothed_counts = {\n",
        "    ('PER', 'John'): 3 + 1,\n",
        "    ('PER', 'Google'): 0 + 1,\n",
        "    ('PER', 'California'): 0 + 1,\n",
        "    ('PER', 'works'): 0 + 1,\n",
        "    ('PER', 'at'): 0 + 1,\n",
        "    ('ORG', 'Google'): 2 + 1,\n",
        "    ('ORG', 'John'): 0 + 1,\n",
        "    ('ORG', 'California'): 0 + 1,\n",
        "    ('ORG', 'works'): 0 + 1,\n",
        "    ('ORG', 'at'): 0 + 1,\n",
        "    ...same for 'LOC' and 'O'\n",
        "}\n",
        "```\n",
        "and the smoothed add-1 probability would be\n",
        "```\n",
        "smoothed_prob = {\n",
        "    ('PER', 'John'): (3 + 1) / (3 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ('PER', 'Google'): (0 + 1) / (3 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ('PER', 'California'): (0 + 1) / (3 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ('PER', 'works'): (0 + 1) / (3 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ('PER', 'at'): (0 + 1) / (3 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ('ORG', 'Google'): (2 + 1) / (2 + 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1),\n",
        "    ...\n",
        "}\n",
        "```\n",
        "Note when you call `apply_smoothing` function later in the HMM, the input `observation_counts` should contain counts for all possible `(curr NER tag, next NER tag)` pairs for transition matrix and `(NER tag, word)` pairs for emission matrix. i.e. if a `(NER tag, word)` doesn't appear in the training data, you should still include it as `observation_counts[(NER tag, word)]=0`.\n",
        "\n",
        "After you complete the function, you may run the following basic test case. Passing this test does NOT guarantee correctness and it is a good idea to write some tests of your own. <br>\n",
        "\n",
        "**NOTE: This example uses entity tags, but you should be using BIO tags in the assignment (ex: B-ORG, I-ORG, B-PER, I-PER, B-LOC, I-LOC, B-MISC, I-MISC, O).**\n",
        "\n",
        "**In your implementation, you should store values in the log space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOo0c4B87CRY"
      },
      "outputs": [],
      "source": [
        "k = 1\n",
        "test_counts = {\n",
        "    ('PER', 'John'): 3,\n",
        "    ('PER', 'Google'): 0,\n",
        "    ('PER', 'California'): 0,\n",
        "    ('PER', 'works'): 0,\n",
        "    ('PER', 'at'): 0,\n",
        "    ('ORG', 'Google'): 2,\n",
        "    ('ORG', 'John'): 0,\n",
        "    ('ORG', 'California'): 0,\n",
        "    ('ORG', 'works'): 0,\n",
        "    ('ORG', 'at'): 0,\n",
        "    ('LOC', 'Google'): 0,\n",
        "    ('LOC', 'John'): 0,\n",
        "    ('LOC', 'California'): 1,\n",
        "    ('LOC', 'works'): 0,\n",
        "    ('LOC', 'at'): 0,\n",
        "    ('O', 'Google'): 0,\n",
        "    ('O', 'John'): 0,\n",
        "    ('O', 'California'): 0,\n",
        "    ('O', 'works'): 4,\n",
        "    ('O', 'at'): 5,\n",
        "}\n",
        "\n",
        "expected_log_smoothed_probs = {\n",
        "    ('PER', 'John'): np.log(4/8),\n",
        "    ('PER', 'Google'): np.log(1/8),\n",
        "    ('PER', 'California'): np.log(1/8),\n",
        "    ('PER', 'works'): np.log(1/8),\n",
        "    ('PER', 'at'): np.log(1/8),\n",
        "    ('ORG', 'Google'): np.log(3/7),\n",
        "    ('ORG', 'John'): np.log(1/7),\n",
        "    ('ORG', 'California'): np.log(1/7),\n",
        "    ('ORG', 'works'): np.log(1/7),\n",
        "    ('ORG', 'at'): np.log(1/7),\n",
        "    ('LOC', 'Google'): np.log(1/6),\n",
        "    ('LOC', 'John'): np.log(1/6),\n",
        "    ('LOC', 'California'): np.log(2/6),\n",
        "    ('LOC', 'works'): np.log(1/6),\n",
        "    ('LOC', 'at'): np.log(1/6),\n",
        "    ('O', 'Google'): np.log(1/14),\n",
        "    ('O', 'John'): np.log(1/14),\n",
        "    ('O', 'California'): np.log(1/14),\n",
        "    ('O', 'works'): np.log(5/14),\n",
        "    ('O', 'at'): np.log(6/14),\n",
        "}\n",
        "\n",
        "vocab = ['John', 'Google', 'California', 'works', 'at']\n",
        "\n",
        "log_smoothed_probs = apply_smoothing(k, test_counts, vocab)\n",
        "\n",
        "assert len(expected_log_smoothed_probs) == len(log_smoothed_probs)\n",
        "for key in test_counts:\n",
        "    np.testing.assert_almost_equal(expected_log_smoothed_probs[key], log_smoothed_probs[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LWxWrxtgTK"
      },
      "source": [
        "### Milestone submission\n",
        "\n",
        "You can make a milestone submission here. You need to submit your `helpers.py' file to hw1-milestone. The output of this function is expected to be a Python dictionary, where the keys are state-observation tuples, and the corresponding values are the log smoothed observation probabilities. We will not check if the values are exactly correct for the milestone submission, but we will run a basic sanity test to check that your returned dictionary does not contain any 0 probability (or -inf log probability values).\n",
        "\n",
        "In particular, we will check to ensure that all (NER tag, word) pairs with 0 observation counts have been included and assigned log smoothed probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_OtHqyw7CRY"
      },
      "source": [
        "### **HMM Class Implementation**\n",
        "---\n",
        "In this section, you will be implementing the HMM class using the following properties.\n",
        "\n",
        "\n",
        "- **documents**: dataset of sentences to train model\n",
        "- **labels**: NER labels corresponding the sentences to train model\n",
        "- **vocab**: dataset vocabulary\n",
        "- **all_tags**: all possible NER tags\n",
        "- **k_t**: add-k parameter to smooth transition probabilities\n",
        "- **k_e**: add-k parameter to smooth emission probabilities\n",
        "- **k_s**: add-k parameter to smooth starting state probabilities\n",
        "- **smoothing_func**: smoothing function to smooth state-observation probabilities\n",
        "\n",
        "\n",
        "In the `model.py` file, complete following methods under the HMM class:\n",
        "1. `build_transition_matrix()`: returns the transition probabilities as a dictionary, mapping all possible (tag, tag) tuple pairs to their corresponding smoothed log probabilities. See function declaration for more details.\n",
        "\n",
        "2. `build_emission_matrix()`:\n",
        "    returns the emission probabilities as a dictionary, mapping all possible\n",
        "    (tag, token) tuple pairs to their corresponding smoothed log probabilities. See function declaration for more details.\n",
        "    \n",
        "3. `get_start_state_probs()`:\n",
        "    returns the starting state probabilities as a dictionary, mapping all possible\n",
        "    tags to their corresponding smoothed log probabilities. See function declaration for more details.\n",
        "      \n",
        "4. `get_tag_likelihood(predicted_tag, previous_tag, document, i)`\n",
        "     returns the tag likelihood used by the Viterbi algorithm for the log probability of a given\n",
        "    predicted_tag conditioned on the previous_tag and document at index i. See function declaration for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1SHlIyFtgTK"
      },
      "source": [
        "To help you check your implementation, we give you a toy example with only 3 tags: [\"B-PER\", \"O\", \"B-LOC\"]. You may run this following basic test case to test your implementation of build_transition_matrix() and build_emission_matrix(). Passing this test does NOT guarantee correctness and it is a good idea to write some tests of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbqu-xwytgTK"
      },
      "outputs": [],
      "source": [
        "toy_hmm_documents = [['Alice', 'and', 'Bob', 'walk', 'in', 'Paris'], ['Bob', 'and', 'Alice', 'walk', 'in', 'Paris']]\n",
        "toy_hmm_vocab = list(set([item for sublist in toy_hmm_documents for item in sublist]))\n",
        "toy_hmm_ner_tags = [['B-PER', 'O', 'B-PER', 'O', 'O', 'B-LOC'], ['B-PER', 'O', 'B-PER', 'O', 'O', 'B-LOC']]\n",
        "print(toy_hmm_ner_tags)\n",
        "print(toy_hmm_vocab)\n",
        "test_hmm_all_tags = list(set([item for sublist in toy_hmm_ner_tags for item in sublist]))\n",
        "test_hmm = HMM(toy_hmm_documents, toy_hmm_ner_tags, toy_hmm_vocab, test_hmm_all_tags,\n",
        "          k_t=1, k_e=1, k_s=1, smoothing_func=apply_smoothing)\n",
        "true_emission_matrix = {\n",
        "    ('B-LOC', 'Bob'): np.log(1/8),\n",
        "    ('B-LOC', 'Alice'): np.log(1/8),\n",
        "    ('B-LOC', 'in'): np.log(1/8),\n",
        "    ('B-LOC', 'and'): np.log(1/8),\n",
        "    ('B-LOC', 'Paris'): np.log(3/8),\n",
        "    ('B-LOC', 'walk'): np.log(1/8),\n",
        "    ('B-PER', 'Bob'): np.log(3/10),\n",
        "    ('B-PER', 'Alice'): np.log(3/10),\n",
        "    ('B-PER', 'in'): np.log(1/10),\n",
        "    ('B-PER', 'and'): np.log(1/10),\n",
        "    ('B-PER', 'Paris'): np.log(1/10),\n",
        "    ('B-PER', 'walk'): np.log(1/10),\n",
        "    ('O', 'Bob'): np.log(1/12),\n",
        "    ('O', 'Alice'): np.log(1/12),\n",
        "    ('O', 'in'): np.log(3/12),\n",
        "    ('O', 'and'): np.log(3/12),\n",
        "    ('O', 'Paris'): np.log(1/12),\n",
        "    ('O', 'walk'): np.log(3/12)\n",
        "}\n",
        "\n",
        "true_transition_matrix = {\n",
        "    ('B-LOC', 'B-LOC'): np.log(1/6),\n",
        "    ('B-LOC', 'B-PER'): np.log(1/6),\n",
        "    ('B-LOC', 'O'): np.log(1/6),\n",
        "    ('B-LOC', 'qf'): np.log(3/6),\n",
        "    ('B-PER', 'B-LOC'): np.log(1/8),\n",
        "    ('B-PER', 'B-PER'): np.log(1/8),\n",
        "    ('B-PER', 'O'): np.log(5/8),\n",
        "    ('B-PER', 'qf'): np.log(1/8),\n",
        "    ('O', 'B-LOC'): np.log(3/10),\n",
        "    ('O', 'B-PER'): np.log(3/10),\n",
        "    ('O', 'O'): np.log(3/10),\n",
        "    ('O', 'qf'): np.log(1/10)\n",
        "}\n",
        "\n",
        "for key in true_emission_matrix:\n",
        "    np.testing.assert_almost_equal(true_emission_matrix[key], test_hmm.emission_matrix[key])\n",
        "for key in true_transition_matrix:\n",
        "    np.testing.assert_almost_equal(true_transition_matrix[key], test_hmm.transition_matrix[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hj4pToAtgTK"
      },
      "source": [
        "If needed, you may extend the above example to test your implementation of get_tag_likelihood and get_start_state_probs functions as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-2qQjzT7CRY"
      },
      "source": [
        "\n",
        "After you completed the class methods, call the HMM constructor to train your model with the following criteria:\n",
        "- Note that the input data to train your model will be the training_data[\"text\"] and training_data[\"NER]\n",
        "- First handle unknown words by using your function `handle_unknown_words` from Part 2.1 using a threshold t = 0.01. You should call this function only on the training dataset in this step.\n",
        "- When you call the HMM constructor, use the following smoothing parameters: k_t = 0.01, k_e = 0.01, k_s = 0.1. You should use your smoothing function that you have implemented in Part 2.2\n",
        "- An example call to the HMM constructor is shown in the example above. Note that the values of all_tags and vocab will be different between the toy example above and your training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKoQ1iHt7CRY"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE8faYCx7CRY"
      },
      "source": [
        "### **Viterbi Implementation**\n",
        "---\n",
        "Implement the function `viterbi(model, observation)` in `viterbi.py` that returns the model's predicted tag sequence for a particular observation. After you have completed the function, use the following cell to see an example tagged sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7hADYvi7CRY"
      },
      "outputs": [],
      "source": [
        "tags = [\"B-ORG\", \"I-ORG\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\", \"O\"]\n",
        "obs = ['The',\n",
        " 'White',\n",
        " 'house',\n",
        " 'located',\n",
        " 'in',\n",
        " 'Ithaca',\n",
        " 'and',\n",
        " 'was',\n",
        " 'founded',\n",
        " 'by',\n",
        " 'Ezra',\n",
        " 'Cornell']\n",
        "\n",
        "# Uncomment and fill out the following line to test your implementation:\n",
        "# viterbi(hmm, obs, tags) #hmm is the trained model from the previous code block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIBZx5Pb7CRY"
      },
      "source": [
        "### **Validation Step for HMM**\n",
        "---\n",
        "Understanding how models perform on unseen data is important, hence the validation set was set aside to be used in the evaluation of the model. In the previous part of the project, we expected you to train your HMM model (i.e., get transition and emission probabilities) on the labeled training data. Next, you will evaluate your trained model on the validation data. Report **Entity Level Mean F1**, which was explained earlier.\n",
        "\n",
        "**TODO:** complete the function `evaluate_model(model, validation_data, tags)` in the `validation.py` file that takes in a model (HMM) and the validation dataset and returns the Entity Level Mean F1. <br>\n",
        "In the `validation.py` file, you have the following helper methods:\n",
        "- **flatten_double_lst(lstlst)**: Takes in a double nested list and returns the flattened version, row-major\n",
        "- **format_output_labels(token_labels, token_indices)**: Takes in a list of token labels and the corresponding list of token indices and returns a dictionary with mapping NER labels (excluding 'O') to indices that have those labels\n",
        "- **mean_f1(y_pred_dict, y_true_dict)**: Takes in two dictionaries (each mapping NER labels (excluding 'O') to indices that have those labels) that represent predicted labels and truth labels respectively and returns the mean f1 score.\n",
        "\n",
        "Below is an example use case of **format_output_labels(token_labels, token_indices)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHNKIF5b7CRY"
      },
      "outputs": [],
      "source": [
        "pred_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\"]\n",
        "true_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"O\", \"O\", \"B-LOC\"]\n",
        "token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
        "\n",
        "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
        "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
        "print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "\n",
        "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQOECYRV7CRY"
      },
      "source": [
        "After you complete `evaluate_model(model, validation_data, tags)`, use the cell below to evaluate the model using the function you have just implemented. Please also take a look into your misclassified cases, as we will be performing error analysis in next part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO0ig2f87CRY"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9iWDJXS7CRY"
      },
      "source": [
        "### HMM Analysis\n",
        "#### Q2.1: In which situations did the system perform effectively, and when did it encounter challenges? For instance, does the model excel in predicting certain NER tags more than others? Could you offer any hypotheses about the reasons behind these patterns and suggest potential improvements?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNjwOrq07CRd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV1fWqgR7CRd"
      },
      "source": [
        "#### Q2.2: How does the treatment of unknown words and the application of smoothing impact the system's performance? Provide examples to illustrate your insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRKDqWj47CRd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xDSVSfPtgTR"
      },
      "source": [
        "## **Submission**\n",
        "\n",
        "You will submit your code and code outputs to two different final assingments (note that the submission to the milestone was described previously):\n",
        "\n",
        "### hw1-programming\n",
        "You will submit the 5 python files: `models.py` `helpers.py` `data_exploration.py` `viterbi.py` and `validation.py` to this assignment. Make sure to include your Name(s) and NetIDs at the appropriate place at the top of these files. You can run `check_submission.py` to check if the format of your files is correct. Note that this does not check the correctness of your implementation; that will be done by the autograder on gradescope.\n",
        "\n",
        "\n",
        "### hw1-checktestperformance\n",
        "\n",
        "Since your HMM model is graded based on its performance on the test set, you will submit your model's predictions on test data to this assignment and see the test score. Each student is allowed to submit to this assignment a maximum of 10 times per day, which resets every midnight.\n",
        "\n",
        "For submission, you will generate a file `output.txt` that will contain the predictions of your HMM model on the test data. Below, we provide a function `create_submission(predictions)` that takes in the predicted labels of the test dataset and outputs it in the correct format in `output.txt`. As a scoring metric on Gradescope, we use **Entity Level Mean F1**.\n",
        "\n",
        "To use `create_submission(predictions)`, you want to first train your HMM model on the training dataset (If you have already done this in previous steps, great!). Then, for each sentence in the testing dataset, run your `viterbi` function using your trained HMM model and append the result to a list. Pass in the list to `create_submission`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIqZUaNLtgTR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "### TODO: Iterate through your test data one document at a time, run viterbi algorithm using your HMM model on that sample and generate predictions. The final output should be (List[List[str]])\n",
        "\n",
        "\n",
        "def create_submission(test_predictions):\n",
        "    \"\"\"\n",
        "    Parameter:\n",
        "    predictions (List[List[str]]):\n",
        "        Prediction results for the test dataset.\n",
        "        It contains a list of a string list.\n",
        "        Each string list corresponds to a test sentence in the test file.\n",
        "    \"\"\"\n",
        "    with open('output.txt', 'w') as file:\n",
        "        file.write(json.dumps(test_predictions))\n",
        "\n",
        "# Uncomment to use\n",
        "# create_submission(test_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}